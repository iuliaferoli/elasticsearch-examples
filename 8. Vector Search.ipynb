{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding our documents\n",
    "\n",
    "Similarily to the previous notebook, we can run another external ML model, in this case dealing with text embeddings, to enrich our indexed data even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "import configparser\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('foobar.ini')\n",
    "cloud_id = config[\"cloud-connection\"][\"cloud_id\"]\n",
    "user = config[\"cloud-connection\"][\"user\"] # by default user = \"elastic\"\n",
    "password = config[\"cloud-connection\"][\"password\"]\n",
    "\n",
    "client = Elasticsearch(\n",
    "    cloud_id=cloud_id,  # cloud id can be found under deployment management\n",
    "    basic_auth=(user, password) # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")\n",
    "\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "index = \"hp_scripts_enriched\"\n",
    "response = client.search(index = index, size=5000)\n",
    "docs = []\n",
    "for line in response[\"hits\"][\"hits\"]:\n",
    "    docs.append({\"text_field\" : line[\"_source\"][\"Sentence\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In your environment terminal run through the following commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the official docker image, add the necessary variables to your environment, and import the necessary model for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull docker.elastic.co/eland/eland:8.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export CLOUD_ID=change_me\n",
    "export USER=change_me\n",
    "export PASSWORD=change_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it --rm docker.elastic.co/eland/eland:8.9.0 \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $CLOUD_ID \\\n",
    "      -u $USER -p $PASSWORD \\\n",
    "      --hub-model-id sentence-transformers/msmarco-MiniLM-L-12-v3 \\\n",
    "      --task-type text_embedding \\\n",
    "      --start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been deployed, you can make inference calls to it, either through the dev console or with our python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST /_ml/trained_models/sentence-transformers__msmarco-minilm-l-12-v3/_infer\n",
    "{\n",
    "  \"docs\": {\n",
    "    \"text_field\": \"I shouldve known that you would be here Professor McGonagall\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".elser_model_1\n",
      "distilbert-base-uncased-finetuned-sst-2-english\n",
      "lang_ident_model_1\n",
      "sentence-transformers__msmarco-minilm-l-12-v3\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.client import MlClient\n",
    "models = MlClient.get_trained_models(client)\n",
    "for model in models[\"trained_model_configs\"]:\n",
    "    print(model[\"model_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
    "\n",
    "doc_test = {\"text_field\": \"I shouldve known that you would be here Professor McGonagall\"}\n",
    "result = MlClient.infer_trained_model(client, model_id =model_id, docs = doc_test)\n",
    "\n",
    "result[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen the inference is working, we can create a new pipeline that will apply the model to our entire index, so we can then use the embeddings in our search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this code in the dev console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipeline\n",
    "client.ingest.put_pipeline(\n",
    "    id=\"embeddings\", \n",
    "    processors=[\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
    "        \"target_field\" : \"text_embedding\",\n",
    "        \"field_map\": {\n",
    "          \"Sentence\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "#creating the new target index, this step can be skipped if you do not want to pre-define custom mappings\n",
    "index = \"hp_scripts_final\"\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "      \"text_embedding.predicted_value\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True,\n",
    "        \"similarity\": \"cosine\"\n",
    "      },\n",
    "      \"Character\": {\n",
    "          \"type\": \"text\"\n",
    "      },\n",
    "      \"Line_number\": {\n",
    "        \"type\": \"long\"\n",
    "      },\n",
    "      \"Sentence\": {\n",
    "        \"type\": \"text\"\n",
    "       },\n",
    "      \"sentiment\": {\n",
    "          \"properties\": {\n",
    "            \"model_id\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"predicted_value\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"prediction_probability\": {\n",
    "              \"type\": \"float\"\n",
    "            }\n",
    "          }\n",
    "        } \n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "client.indices.create(index=index, mappings=mappings)\n",
    "\n",
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"hp_scripts_enriched\"},\n",
    "      \"dest\": {\n",
    "    \"index\": \"hp_scripts_final\",\n",
    "    \"pipeline\": \"embeddings\"\n",
    "    }}, wait_for_completion = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'hp_scripts_final'\n",
    "response = client.search(index = index)\n",
    "\n",
    "line = response[\"hits\"][\"hits\"][0]\n",
    "print(line['_source'][\"Sentence\"])\n",
    "print(line['_source'][\"text_embedding\"][\"predicted_value\"][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search through our text now by embedding our queries with the same model and using the newly build vectors as part of our search ceriteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with some basic associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = {\"text_field\" : \"magic\"}\n",
    "question2 = {\"text_field\" : \"wand\"}\n",
    "question3 = {\"text_field\" : \"harry potter\"}\n",
    "question4 = {\"text_field\" : \"dumbledore\"}\n",
    "question_list = [question1, question2, question3, question4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(question_list):\n",
    "    answer_list = []\n",
    "    index = \"hp_scripts_final\"\n",
    "\n",
    "    for question in question_list:\n",
    "        result = MlClient.infer_trained_model(client, model_id =model_id, docs = question)\n",
    "        query_vector = result[\"inference_results\"][0][\"predicted_value\"]\n",
    "\n",
    "        query = {\n",
    "        \"field\": \"text_embedding.predicted_value\",\n",
    "        \"query_vector\": query_vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 100\n",
    "        }\n",
    "        \n",
    "\n",
    "        result = client.search(index = index, knn=query, source=[\"Sentence\", \"Character\"])\n",
    "        answer = []\n",
    "        \n",
    "        answer.append(\"prompt: \" + question[\"text_field\"])\n",
    "\n",
    "        for element in result[\"hits\"][\"hits\"]:\n",
    "            answer.append(\"{}: {}, score {}\".format(element[\"_source\"][\"Character\"], element[\"_source\"][\"Sentence\"], element[\"_score\"]))\n",
    "\n",
    "        answer_list.append(answer)\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prompt: magic',\n",
       "  'Vernon: Theres no such thing as magic, score 0.8299141',\n",
       "  'Hermione: Oh are you doing magic, score 0.8199536',\n",
       "  'GILDEROY LOCKHART: This is just like magic, score 0.8000889',\n",
       "  'Hagrid: Strickly speaking Im not allowed to do magic, score 0.77473605',\n",
       "  'HARRY: Im not allowed to use magic outside of school, score 0.7666023'],\n",
       " ['prompt: wand',\n",
       "  'GILDEROY LOCKHART: Wands at the ready, score 0.8621737',\n",
       "  'Harry: I still need a wand, score 0.8611474',\n",
       "  'Harry: And who owned that wand, score 0.8589636',\n",
       "  'RON: My wand Look at my wand, score 0.843747',\n",
       "  'LUPIN: Wand at the ready Ron, score 0.832528'],\n",
       " ['prompt: harry potter',\n",
       "  'Whispers: Harry Potter, score 1.0',\n",
       "  'McGonagall: Harry Potter, score 1.0',\n",
       "  'McGonagall: Harry Potter, score 1.0',\n",
       "  'VERNON: Harry Potter, score 1.0',\n",
       "  'PHOTOGRAPHER: Harry Potter, score 1.0'],\n",
       " ['prompt: dumbledore',\n",
       "  'Hermione: Dumbledore, score 1.0',\n",
       "  'HARRY: Dumbledore, score 1.0',\n",
       "  'LUCIUS MALFOY: Dumbledore, score 1.0',\n",
       "  'FRED: Dumbledore, score 1.0',\n",
       "  'HARRY: But Dumbledore, score 0.9646542']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it just behaves as as a \"normal\" keyword search. Especially the names are quite bad - just finding the full name in a line. \n",
    "\n",
    "Let's try some more complex searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = {\"text_field\" : \"what do they fear\"}\n",
    "question2 = {\"text_field\" : \"bad decisions\"}\n",
    "question3 = {\"text_field\" : \"breaking rules\"}\n",
    "question4 = {\"text_field\" : \"when are you brave\"}\n",
    "question_list = [question1, question2, question3, question4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prompt: what do they fear',\n",
       "  'LUPIN: That suggests what you fear the most is fear itself, score 0.84556496',\n",
       "  'Hagrid: Anything you couldnt explain when you were angry or scared, score 0.7971493',\n",
       "  'LUPIN: Concentrate Face your fear, score 0.7910902',\n",
       "  'MR WEASLEY: You are in danger, score 0.7876258',\n",
       "  'MCGONAGALL: Our worst fear has been realized, score 0.786567'],\n",
       " ['prompt: bad decisions',\n",
       "  'Ron: Its bad, score 0.772177',\n",
       "  'Harry: But Hagrid there must be a mistake, score 0.7552644',\n",
       "  'Harry: No youve made a mistake, score 0.7539615',\n",
       "  'RON: Hermione Bad idea Bad idea, score 0.7443874',\n",
       "  'Dumbledore: The good and the bad, score 0.7368764'],\n",
       " ['prompt: breaking rules',\n",
       "  'HERMIONE: Not to mention wed be breaking about 50 school rules, score 0.75131655',\n",
       "  'DUMBLEDORE: You both realize of course that in the past few hours you have broken perhaps a dozen school rules, score 0.7429861',\n",
       "  'Vernon: You are breaking and entering, score 0.7344173',\n",
       "  'McGonagall: Any rulebreaking and you will lose points, score 0.7222869',\n",
       "  'MCGONAGALL: No exceptions, score 0.72045773'],\n",
       " ['prompt: when are you brave',\n",
       "  'LUPIN: Be brave, score 0.8656337',\n",
       "  'Sorting Hat: Plenty of courage I see, score 0.7475611',\n",
       "  'Hermione: Friendship and bravery, score 0.7310822',\n",
       "  'AUNT MARGE: Dont you dare, score 0.7302633',\n",
       "  'LUCIUS MALFOY: How dare you, score 0.72272116']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally some nicer results! \n",
    "* Fear associated with other emotions, especially being scared or scarry.\n",
    "* Bad decisions implying a mistake\n",
    "* Some gramatical flexibility like broken / breakging / rulebreaking and the associated exceptions\n",
    "* Bravery linked to courage and daring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use our book knowledge to navigate this. Like getting the model to guess a particular result we know is in the books. \n",
    "\n",
    "Can you guess these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = {\"text_field\" : \"who is the smartest\"}\n",
    "question2 = {\"text_field\" : \"jailbreak\"}\n",
    "question3 = {\"text_field\" : \"room that is hidden\"}\n",
    "question4 = {\"text_field\" : \"evil person\"}\n",
    "question_list = [question1, question2, question3, question4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search(question_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
