{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Eland\n",
    "### You know, for data science.\n",
    "\n",
    "We've explored the main Python client for Elastic in the [previous notebook](/5.%20Python%20Wrapper.ipynb). This allows us to more easily make calls to the Elastic engine via the API directly from our notebooks or projects rather than the Elastic console or another HTTP request tool.\n",
    "\n",
    "However, when we start explorin more advanced concepts and features, we can also leverage the Eland Elasticserch Python Client. This has been design to make data exploration and analysis easier, as well as allow us to bring in compatible ML models and use them for inference in Elasticsearch.\n",
    "\n",
    "\n",
    "RSe the [Eland docs here]https://eland.readthedocs.io/en/latest/index.html)\n",
    "Check out this awesome blog series [about using NLP in elastic here](https://www.elastic.co/blog/how-to-deploy-nlp-named-entity-recognition-ner-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install 'eland' via pip:\n",
    "```\n",
    "pip install eland \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will connect to Elaticsearch again with the Python Client as we did before. Then we will be able to connect to data from Elastisearch, and see it in the native Eland Dataframe. See comparison between this and a pandas dataframe [here](https://eland.readthedocs.io/en/v8.9.0/examples/demo_notebook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to both Elasticsearch Python Clients - Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass  # For securely getting user input\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY # API keys can be generated under management / security\n",
    ")\n",
    "\n",
    "index = \"hp_scripts\"\n",
    "\n",
    "ed_hp_script = ed.DataFrame(client, es_index_pattern=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary that fits the required input for the NLP models we will want to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.search(index = index, size=5000)\n",
    "docs = []\n",
    "for line in response[\"hits\"][\"hits\"]:\n",
    "    docs.append({\"text_field\" : line[\"_source\"][\"Sentence\"]})\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the data we've indexed in the previous notebook in our Eland Dataframe. \n",
    "\n",
    "Next up, let's import some models to play with on this data.\n",
    "\n",
    "Eland allows transforming trained models from scikit-learn, XGBoost, and LightGBM libraries to be serialized and used as an inference model in Elasticsearch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Docker for Eland\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NLP use cases we can import [models from the Hugging Face model hub](https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english)\n",
    "\n",
    "Explanation of using docker for this. \n",
    "\n",
    "The minimum dedicated ML node size for deploying and using the natural language processing models is 16 GB in Elasticsearch Service "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone git@github.com:elastic/eland.git\n",
    "cd eland\n",
    "docker build -t elastic/eland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, since more recently you can pull the official docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull docker.elastic.co/eland/eland:8.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it --rm elastic/eland \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $CLOUD_ID \\\n",
    "      -u $USER -p $PASSWORD \\\n",
    "      --hub-model-id distilbert-base-uncased-finetuned-sst-2-english \\\n",
    "      --task-type text_classification \\\n",
    "      --start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make calls that use the deployed model using this structure:\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/get-trained-models.html?#ml-get-trained-models-request \n",
    "\n",
    "As a quick example, this is what the query looks like when used diretly through the API requests. \n",
    "From now on, we will continue to use the python client instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST _ml/trained_models/distilbert-base-uncased-finetuned-sst-2-english/_infer\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"text_field\": \"The movie was awesome!\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with ML Models in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.client import MlClient\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "models = MlClient.get_trained_models(client, model_id=model_id)\n",
    "models.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = MlClient.get_trained_models_stats(client, model_id=model_id)\n",
    "stats.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = MlClient.infer_trained_model(client, model_id=model_id, docs=docs[0:10], timeout=None)\n",
    "#queue capacity defaults to 1024 so bulk requests should be split\n",
    "\n",
    "response[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = {\"text_field\": \"I shouldve known that you would be here Professor McGonagall\"}\n",
    "result = MlClient.infer_trained_model(client, model_id =model_id, docs = doc_test)\n",
    "\n",
    "result[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks to be running smoothly! \n",
    "\n",
    "We can now apply the model to our entire index by using an Elastic pipeline. \n",
    "\n",
    "Here you can specify which transformations or enrichments to apply on all your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an enrcihment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipeline\n",
    "client.ingest.put_pipeline(\n",
    "    id=\"sentiment\", \n",
    "    processors=[\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"target_field\" : \"sentiment\",\n",
    "        \"field_map\": {\n",
    "          \"Sentence\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "#creating the new target index, this step can be skipped if you do not want to pre-define custom mappings\n",
    "index = \"hp_scripts_enriched\"\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "      \"Character\": {\n",
    "          \"type\": \"text\"\n",
    "      },\n",
    "      \"Line_number\": {\n",
    "        \"type\": \"long\"\n",
    "      },\n",
    "      \"Sentence\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"sentiment.predicted_value\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "      },\n",
    "      \"sentiment.prediction_probability\": {\n",
    "          \"type\": \"float\"\n",
    "      }    \n",
    "    }\n",
    "  }\n",
    "\n",
    "client.indices.create(index=index, mappings=mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"hp_scripts\"},\n",
    "      \"dest\": {\"index\": \"hp_scripts_enriched\", \"pipeline\" : \"sentiment\"}\n",
    "    }, wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.search(index = \"hp_scripts_enriched\")\n",
    "\n",
    "print(\"We get back {total} results, here are the top ones:\".format(total=response[\"hits\"]['total']['value']))\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"Sentence\"], hit['_source'][\"sentiment\"][\"predicted_value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again add more arguments to our searches to make it more interesting. \n",
    "\n",
    "For instance, the most negative lines in the first three movies would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"match\" : {\n",
    "      \"sentiment.predicted_value\": \"NEGATIVE\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "response = client.search(index = \"hp_scripts_enriched\",query=query, sort=\"sentiment.prediction_probability:desc\")\n",
    "\n",
    "print(\"The most negative sentences in the series:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"Sentence\"],  hit['_source'][\"sentiment\"][\"prediction_probability\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"match\" : {\n",
    "      \"sentiment.predicted_value\": \"POSITIVE\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "response = client.search(index = \"hp_scripts_enriched\",query=query, sort=\"sentiment.prediction_probability:desc\")\n",
    "\n",
    "print(\"The most positive sentences in the series:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"Sentence\"],  hit['_source'][\"sentiment\"][\"prediction_probability\"] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
